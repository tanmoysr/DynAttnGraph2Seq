{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/omarsar/pytorch_neural_machine_translation_attention/blob/master/NMT_in_PyTorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8 6 9 2 8 6 9 2 3 3 2 2 1 1 2 2 2 6 2 6 2 1 2 ...</td>\n",
       "      <td>Dx BCG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1 2 1 2 6 6 2 2 2 2 2 2 6 6</td>\n",
       "      <td>Dx TURBT Dx BCG BCG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5 5 4 4 3 3 6 6 3 6 3 9 3 9 3 9 1 1 7 3 3 3 9 ...</td>\n",
       "      <td>Dx Surgery Surgery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5 5 1 1 2 2 2 2 3 3 3 2 2 3 5 4 2 3 5 4 2 3 2 ...</td>\n",
       "      <td>Dx TURBT BCG TURBT Surgery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2 2 2 2</td>\n",
       "      <td>Dx TURBT TURBT BCG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  8 6 9 2 8 6 9 2 3 3 2 2 1 1 2 2 2 6 2 6 2 1 2 ...   \n",
       "1                        1 2 1 2 6 6 2 2 2 2 2 2 6 6   \n",
       "2  5 5 4 4 3 3 6 6 3 6 3 9 3 9 3 9 1 1 7 3 3 3 9 ...   \n",
       "3  5 5 1 1 2 2 2 2 3 3 3 2 2 3 5 4 2 3 5 4 2 3 2 ...   \n",
       "4                                            2 2 2 2   \n",
       "\n",
       "                         trgt  \n",
       "0                      Dx BCG  \n",
       "1         Dx TURBT Dx BCG BCG  \n",
       "2          Dx Surgery Surgery  \n",
       "3  Dx TURBT BCG TURBT Surgery  \n",
       "4          Dx TURBT TURBT BCG  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bladder Cancer Data\n",
    "f_src = open('train.src', encoding='UTF-8').read().strip().split('\\n')\n",
    "f_tgt = open('train.tgt', encoding='UTF-8').read().strip().split('\\n')\n",
    "dic_data = {'src':f_src,'trgt':f_tgt}\n",
    "data = pd.DataFrame(dic_data)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_src = open('vocab.src', encoding='UTF-8').read().strip().split('\\n')\n",
    "f_tgt = open('vocab.tgt', encoding='UTF-8').read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # English to Spanish data\n",
    "# f = open('spa.txt', encoding='UTF-8').read().strip().split('\\n')\n",
    "# lines = f\n",
    "# # sample size (try with smaller sample size to reduce computation)\n",
    "# num_examples = 300\n",
    "# # creates lists containing each pair\n",
    "# original_word_pairs = [[w for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "# data = pd.DataFrame(original_word_pairs, columns=[\"eng\", \"es\"])\n",
    "# data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    \"\"\"\n",
    "    Normalizes latin chars with accent to their canonical decomposition\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    \n",
    "    w = w.rstrip().strip()\n",
    "    \n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>trgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>&lt;start&gt; 1 1 2 2 5 2 5 2 2 2 1 1 2 2 3 3 2 5 9 ...</td>\n",
       "      <td>&lt;start&gt; dx bcg bcg &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>&lt;start&gt; 6 6 3 3 &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; surgery &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>&lt;start&gt; 5 5 1 1 2 2 2 2 3 3 3 2 2 3 5 4 2 3 5 ...</td>\n",
       "      <td>&lt;start&gt; dx turbt bcg turbt surgery &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>&lt;start&gt; 3 3 3 4 1 9 2 3 4 1 2 9 9 4 3 6 2 1 6 ...</td>\n",
       "      <td>&lt;start&gt; dx &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>&lt;start&gt; 2 2 2 2 &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; turbt bcg &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>&lt;start&gt; 9 9 6 6 1 &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; surgery &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>&lt;start&gt; 1 6 1 6 6 1 3 8 2 1 8 3 7 3 7 1 1 6 6 ...</td>\n",
       "      <td>&lt;start&gt; chemotherapy surgery &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>&lt;start&gt; 3 &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; surgery dx chemotherapy &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>&lt;start&gt; 5 5 3 3 5 9 9 5 1 3 5 1 3 8 8 1 1 1 3 ...</td>\n",
       "      <td>&lt;start&gt; chemotherapy &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>&lt;start&gt; 3 3 1 4 1 2 4 4 2 2 4 5 5 1 1 1 1 &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; dx turbt bcg bcg &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  src  \\\n",
       "75  <start> 1 1 2 2 5 2 5 2 2 2 1 1 2 2 3 3 2 5 9 ...   \n",
       "58                              <start> 6 6 3 3 <end>   \n",
       "3   <start> 5 5 1 1 2 2 2 2 3 3 3 2 2 3 5 4 2 3 5 ...   \n",
       "38  <start> 3 3 3 4 1 9 2 3 4 1 2 9 9 4 3 6 2 1 6 ...   \n",
       "85                              <start> 2 2 2 2 <end>   \n",
       "32                            <start> 9 9 6 6 1 <end>   \n",
       "6   <start> 1 6 1 6 6 1 3 8 2 1 8 3 7 3 7 1 1 6 6 ...   \n",
       "87                                    <start> 3 <end>   \n",
       "71  <start> 5 5 3 3 5 9 9 5 1 3 5 1 3 8 8 1 1 1 3 ...   \n",
       "43    <start> 3 3 1 4 1 2 4 4 2 2 4 5 5 1 1 1 1 <end>   \n",
       "\n",
       "                                        trgt  \n",
       "75                  <start> dx bcg bcg <end>  \n",
       "58                     <start> surgery <end>  \n",
       "3   <start> dx turbt bcg turbt surgery <end>  \n",
       "38                          <start> dx <end>  \n",
       "85                   <start> turbt bcg <end>  \n",
       "32                     <start> surgery <end>  \n",
       "6         <start> chemotherapy surgery <end>  \n",
       "87     <start> surgery dx chemotherapy <end>  \n",
       "71                <start> chemotherapy <end>  \n",
       "43            <start> dx turbt bcg bcg <end>  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we do the preprocessing using pandas and lambdas\n",
    "data[\"src\"] = data.src.apply(lambda w: '<start> ' + w + ' <end>')\n",
    "data[\"trgt\"] = data.trgt.apply(lambda w: preprocess_sentence(w))\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        \n",
    "        self.create_index()\n",
    "        \n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            # update with individual tokens\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "            \n",
    "        # sort the vocab\n",
    "        self.vocab = sorted(self.vocab)\n",
    "\n",
    "        # add a padding token with index 0\n",
    "        self.word2idx['<pad>'] = 0\n",
    "        \n",
    "        # word to index mapping\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
    "        \n",
    "        # index to word mapping\n",
    "        for word, index in self.word2idx.items():\n",
    "            self.idx2word[index] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train_src, train_tgt,inp_lang,targ_lang):\n",
    "    # Bladder Cancer Data\n",
    "    f_src = open(train_src, encoding='UTF-8').read().strip().split('\\n')\n",
    "    f_tgt = open(train_tgt, encoding='UTF-8').read().strip().split('\\n')\n",
    "    dic_data = {'src':f_src,'trgt':f_tgt}\n",
    "    data = pd.DataFrame(dic_data)\n",
    "    # Now we do the preprocessing using pandas and lambdas\n",
    "    data[\"src\"] = data.src.apply(lambda w: '<start> ' + w + ' <end>')\n",
    "    data[\"trgt\"] = data.trgt.apply(lambda w: preprocess_sentence(w))\n",
    "    # Vectorize the input and target languages\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in src.split(' ')]  for src in data[\"src\"].values.tolist()]\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in trgt.split(' ')]  for trgt in data[\"trgt\"].values.tolist()]\n",
    "    # calculate the max_length of input and output tensor\n",
    "    max_length_inp, max_length_tar = data_processor.max_length(input_tensor), data_processor.max_length(target_tensor)\n",
    "\n",
    "    # inplace padding\n",
    "    input_tensor = [data_processor.pad_sequences(x, max_length_inp) for x in input_tensor]\n",
    "    target_tensor = [data_processor.pad_sequences(x, max_length_tar) for x in target_tensor]\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 5, 3, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# index language using the class above\n",
    "inp_lang = LanguageIndex(data[\"src\"].values.tolist())\n",
    "targ_lang = LanguageIndex(data[\"trgt\"].values.tolist())\n",
    "# Vectorize the input and target languages\n",
    "input_tensor = [[inp_lang.word2idx[s] for s in src.split(' ')]  for src in data[\"src\"].values.tolist()]\n",
    "target_tensor = [[targ_lang.word2idx[s] for s in trgt.split(' ')]  for trgt in data[\"trgt\"].values.tolist()]\n",
    "target_tensor[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targ_lang.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the max_length of input and output tensor\n",
    "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_sequences(x, max_len):\n",
    "    padded = np.zeros((max_len), dtype=np.int64)\n",
    "    if len(x) > max_len: padded[:] = x[:max_len]\n",
    "    else: padded[:len(x)] = x\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inplace padding\n",
    "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
    "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
    "len(target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 74, 19, 19)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to tensors and pass to the Dataloader \n",
    "# to create an batch iterator\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        # TODO: convert this into torch code is possible\n",
    "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        x_len = self.length[index]\n",
    "        return x,y,x_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
    "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
    "\n",
    "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
    "                     drop_last=True,\n",
    "                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
    "        \n",
    "    def forward(self, x, lens, device):\n",
    "        # x: batch_size, max_length \n",
    "        \n",
    "        # x: batch_size, max_length, embedding_dim\n",
    "        x = self.embedding(x) \n",
    "                \n",
    "        # x transformed = max_len X batch_size X embedding_dim\n",
    "        # x = x.permute(1,0,2)\n",
    "        x = pack_padded_sequence(x, lens) # unpad\n",
    "    \n",
    "        self.hidden = self.initialize_hidden_state(device)\n",
    "        \n",
    "        # output: max_length, batch_size, enc_units\n",
    "        # self.hidden: 1, batch_size, enc_units\n",
    "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
    "        \n",
    "        # pad the sequence to the max length in the batch\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        \n",
    "        return output, self.hidden\n",
    "\n",
    "    def initialize_hidden_state(self, device):\n",
    "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sort batch function to be able to use with pad_packed_sequence\n",
    "def sort_batch(X, y, lengths):\n",
    "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
    "    X = X[indx]\n",
    "    y = y[indx]\n",
    "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([546, 64, 1024])\n"
     ]
    }
   ],
   "source": [
    "### Testing Encoder part\n",
    "# TODO: put whether GPU is available or not\n",
    "# Device\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "encoder.to(device)\n",
    "# obtain one sample from the data iterator\n",
    "it = iter(dataset)\n",
    "x, y, x_len = next(it)\n",
    "\n",
    "# sort the batch first to be able to use with pac_pack_sequence\n",
    "xs, ys, lens = sort_batch(x, y, x_len)\n",
    "# xs, ys, lens = x, y, x_len\n",
    "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
    "\n",
    "print(enc_output.size()) # max_length, batch_size, enc_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.enc_units = enc_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
    "                          self.dec_units,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
    "        self.V = nn.Linear(self.enc_units, 1)\n",
    "    \n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        # enc_output original: (max_length, batch_size, enc_units)\n",
    "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
    "        enc_output = enc_output.permute(1,0,2)\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
    "        \n",
    "        # score: (batch_size, max_length, hidden_size)\n",
    "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
    "          \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # ? Looks like attention vector in diagram of source\n",
    "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output =  output.view(-1, output.size(2))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return torch.zeros((1, self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([64, 546])\n",
      "Output:  torch.Size([64, 12])\n",
      "Encoder Output:  torch.Size([425, 64, 1024])\n",
      "Encoder Hidden:  torch.Size([1, 64, 1024])\n",
      "Decoder Input:  torch.Size([64, 1])\n",
      "--------\n",
      "Prediction:  torch.Size([64, 8])\n",
      "Decoder Hidden:  torch.Size([1, 64, 1024])\n",
      "torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "encoder.to(device)\n",
    "# obtain one sample from the data iterator\n",
    "it = iter(dataset)\n",
    "x, y, x_len = next(it)\n",
    "\n",
    "print(\"Input: \", x.shape)\n",
    "print(\"Output: \", y.shape)\n",
    "\n",
    "# sort the batch first to be able to use with pac_pack_sequence\n",
    "xs, ys, lens = sort_batch(x, y, x_len)\n",
    "\n",
    "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
    "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
    "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "#print(enc_hidden.squeeze(0).shape)\n",
    "\n",
    "dec_hidden = enc_hidden#.squeeze(0)\n",
    "dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
    "print(\"Decoder Input: \", dec_input.shape)\n",
    "print(\"--------\")\n",
    "\n",
    "for t in range(1, y.size(1)):\n",
    "    # enc_hidden: 1, batch_size, enc_units\n",
    "    # output: max_length, batch_size, enc_units\n",
    "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
    "                                         dec_hidden.to(device), \n",
    "                                         enc_output.to(device))\n",
    "    \n",
    "    print(\"Prediction: \", predictions.shape)\n",
    "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
    "    \n",
    "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
    "    \n",
    "    dec_input = y[:, t].unsqueeze(1)\n",
    "    print(dec_input.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
    "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
    "    #print(mask)\n",
    "#     mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
    "    mask = real.ge(1).to(device).float()\n",
    "    \n",
    "    loss_ = criterion(pred, real) * mask \n",
    "    return torch.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "## TODO: Combine the encoder and decoder into one class\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
    "                       lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "Epoch 1 Batch 0 Loss 0.2855\n",
      "Epoch 1 Loss 0.2855\n",
      "Time taken for 1 epoch 45.234190464019775 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
    "        print('batch')\n",
    "        loss = 0\n",
    "        \n",
    "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
    "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
    "        pred_all=[]\n",
    "        for t in range(1, ys.size(1)):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
    "                                         dec_hidden.to(device), \n",
    "                                         enc_output.to(device))\n",
    "            pred_all.append(predictions)\n",
    "            print(targ_lang.idx2word[predictions[0].topk(1)[-1][0].item()],\n",
    "                 targ_lang.idx2word[predictions[1].topk(1)[-1][0].item()],\n",
    "                 targ_lang.idx2word[predictions[2].topk(1)[-1][0].item()],\n",
    "                 targ_lang.idx2word[predictions[3].topk(1)[-1][0].item()],\n",
    "                 targ_lang.idx2word[predictions[4].topk(1)[-1][0].item()],\n",
    "                 targ_lang.idx2word[predictions[5].topk(1)[-1][0].item()],\n",
    "                 targ_lang.idx2word[predictions[6].topk(1)[-1][0].item()],\n",
    "                 targ_lang.idx2word[predictions[7].topk(1)[-1][0].item()])\n",
    "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
    "            #loss += loss_\n",
    "            dec_input = ys[:, t].unsqueeze(1)\n",
    "            \n",
    "        \n",
    "        batch_loss = (loss / int(ys.size(1)))\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.detach().item()))\n",
    "        \n",
    "        \n",
    "    ### TODO: Save checkpoint for model\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.topk(1)[-1][0].item()\n",
    "targ_lang.idx2word[predictions.topk(1)[-1][0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 0, 0, 4, 1, 0, 0, 1, 0, 0, 7, 0, 3, 5, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 7, 0, 0, 0, 1, 0, 1, 7, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[:, t-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions[0].topk(1)[-1][0].item()\n",
    "sample_seq={}\n",
    "for i in range(0,pred_all[0].shape[0]): # batch size\n",
    "    if i not in sample_seq.keys():\n",
    "        sample_seq[i]=[]\n",
    "    for j in range(0,len(pred_all)):\n",
    "        seq=targ_lang.idx2word[pred_all[j][i,:].topk(1)[-1][0].item()]\n",
    "        if seq not in ['<end>', '<pad>','<start>']:\n",
    "            sample_seq[i].append(seq)\n",
    "\n",
    "for key, value in enumerate(sample_seq):\n",
    "    print(sample_seq[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dx', 'surgery', 'surgery']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg', 'surgery']\n",
      "['dx', 'surgery', 'surgery', 'surgery']\n",
      "['dx', 'bcg']\n",
      "['surgery']\n",
      "['surgery', 'surgery', 'bcg', 'surgery']\n",
      "['dx']\n",
      "['surgery', 'bcg', 'bcg', 'bcg']\n",
      "['surgery', 'surgery']\n",
      "['dx', 'bcg', 'bcg', 'bcg']\n",
      "['dx', 'bcg', 'bcg']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg', 'bcg']\n",
      "['dx']\n",
      "['surgery', 'surgery']\n",
      "['surgery']\n",
      "['dx']\n",
      "['dx', 'bcg', 'bcg', 'bcg']\n",
      "['dx', 'surgery']\n",
      "['dx', 'bcg']\n",
      "['surgery']\n",
      "['surgery']\n",
      "['dx', 'bcg', 'bcg']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg']\n",
      "['dx', 'bcg', 'bcg', 'bcg', 'bcg']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx', 'bcg', 'bcg', 'bcg']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx', 'bcg', 'bcg']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg', 'bcg']\n",
      "['dx']\n",
      "['dx', 'bcg', 'bcg', 'bcg']\n",
      "['dx', 'bcg', 'bcg']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg']\n",
      "['dx', 'bcg', 'bcg']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx']\n",
      "['dx', 'bcg']\n",
      "['dx']\n",
      "['dx', 'surgery']\n"
     ]
    }
   ],
   "source": [
    "for key, value in enumerate(sample_seq):\n",
    "    print(sample_seq[value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<end>': 1,\n",
       " '<start>': 2,\n",
       " 'bcg': 3,\n",
       " 'chemotherapy': 4,\n",
       " 'dx': 5,\n",
       " 'surgery': 6,\n",
       " 'turbt': 7}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targ_lang.word2idx['<end>'] #1\n",
    "# targ_lang.word2idx['<pad>'] #0\n",
    "targ_lang.word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 12])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_all[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.array(pred_all).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # sample_seq = {}\n",
    "    # for i in range(0, pred_all[0].shape[0]):  # batch size\n",
    "    #     if i not in sample_seq.keys():\n",
    "    #         sample_seq[i] = []\n",
    "    #     for j in range(0, len(pred_all)):\n",
    "    #         seq = targ_lang.idx2word[pred_all[j][i, :].topk(1)[-1][0].item()]\n",
    "    #         if seq not in ['<end>', '<pad>', '<start>']:\n",
    "    #             sample_seq[i].append(seq)\n",
    "    # for key, value in enumerate(sample_seq):\n",
    "    #     print(sample_seq[value])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
